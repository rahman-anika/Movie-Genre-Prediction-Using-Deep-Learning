# -*- coding: utf-8 -*-
"""MOVIE_GENRE_PREDICTION_USING_DEEP_LEARNING.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rB56I5eqKHfdeSb6Eu0ZRCvxTk__omA6
"""

from google.colab import drive
drive.mount('/content/drive')

project_path = '/content/drive/MyDrive/Project/'
data_path = '/content/drive/MyDrive/Project/trainingSet.csv'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sortedcontainers import SortedList, SortedSet, SortedDict

"""

**1. Reading Dataset**

"""

df = pd.read_csv(data_path)
df.drop('Genre2',axis='columns', inplace=True)
df.drop('Genre3',axis='columns', inplace=True)

"""

**2. Dataset Analysis**

"""

df

distinct_genres = SortedSet() #ordered set of distict element
for i in range(1, 2):
  column = "Genre" + str(i)
  for x in df[column]:
    gnr = str(x)
    gnr = gnr.lower()
    gnr = gnr.strip()
    gnr = gnr.strip(' ')
    if (str(gnr) != "nan"):
      distinct_genres.add(gnr)
#print('In this dataset there are', len(distinct_genres), 'distinct genres')
all_genres = list(distinct_genres)

"""**In this dataset there are 19 distinct genres** <br> <br>


1.Animation<br> 
2.Action<br>
3.Comedy<br>
4.Adventure<br>
5.Biography<br>
6.Drama<br>
7.Crime<br>
8.Fantasy<br>
9.Mystery<br>
10.Romance<br>
11.Sci-Fi<br>
12.Documentary<br>
13.Family<br>
14.Horror<br>
15.Thriller<br>
16.Short<br>
17.Western<br>
18.War<br>
19.Musical<br>


"""

all_genres_count = {}       # counting every genres
for i in range(1, 2):
  column = "Genre" + str(i)
  for x in df[column]:
    gnr = str(x)
    gnr = gnr.lower()
    gnr = gnr.strip()
    gnr = gnr.strip(' ')
    if (str(gnr) != "nan"):
      if (all_genres_count.get(gnr) == None):
        all_genres_count[gnr] = 1
      else :
        all_genres_count[gnr] += 1
#print('In this dataset there are', len(distinct_genres), 'distinct genres')

plot_column_name = []
plot_height = []
for name, cnt in all_genres_count.items():
  plot_column_name.append(name)
  plot_height.append(cnt)

x = np.arange(len(plot_height))
width = .75
fig, ax = plt.subplots()
fig.set_figheight(8)
fig.set_figwidth(22)
rects1 = ax.bar(x, plot_height, width, color=(0.0, 0.0, 0.9, 0.6))
for i, v in enumerate(plot_height):
    plt.text(x[i] - width / 2, v, str(v))
plt.xticks(x, plot_column_name, rotation='vertical')
plt.legend()

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

import os
from os import path
import shutil
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset
from torch.utils.data.sampler import SubsetRandomSampler
from torchvision import datasets, transforms, models
import matplotlib
import matplotlib.pyplot as plt
from PIL import Image
import threading
import random

class Custom_Count_Vectorizer:
  """instance"""
  my_vectorizer = None
  word_vector = None
  dataset = None

  """methods"""
  #@ initalize the vocabulary based on the given corpus and make a dataset with given labels
  def __init__(self, corpus, corpus_label, vectorizer = 'COUNT-B'):
    #@initialize the vectorizer
    if (vectorizer == 'COUNT-B'):
      self.my_vectorizer = CountVectorizer(stop_words='english', binary=True)
      print('Using Binary Count Vectorizer')
    elif (vectorizer == 'COUNT'):
      self.my_vectorizer = CountVectorizer(stop_words='english')
      print('Using Count Vectorizer')
    elif (vectorizer == 'TF-IDF'):
      self.my_vectorizer = TfidfVectorizer(stop_words='english')
      print('Using TF-IDF Vectorizer')
    else:
      self.my_vectorizer = CountVectorizer(stop_words='english', binary=True)
      print('Can not use the specific vectorizer.\nUsing Binary Count Vectorizer as Default')

    self.word_vector = self.my_vectorizer.fit_transform(corpus).toarray()
    assert(len(self.word_vector) == len(corpus_label))
    self.dataset = []
    for i in range(len(self.word_vector)):
      self.dataset.append((torch.FloatTensor(self.word_vector[i]) / corpus_label[i][1], corpus_label[i][0]))

  #@ returns a word vector for a new text, based on the vocabulary
  def get_vector(self, ar):
    return self.my_vectorizer.transform([ar]).toarray()[0]
  
  #@ retruns a word vector for a new text, based on the vocabulary in a tensor form
  def get_tensor(self, ar):
    return torch.FloatTensor(self.my_vectorizer.transform([ar]).toarray()[0])

  #@ returns a dataset with labels, based on the corpus that was given during class initialization
  def get_dataset(self, shuffle = False):
    if (shuffle):
      random.shuffle(self.dataset)
    return self.dataset

label_to_int = {}
int_to_label = {}
given_label = 0
for name in all_genres_count.keys():
  label_to_int[name] = given_label
  int_to_label[given_label] = name
  given_label += 1

corpus = []
corpus_label = []
def make_corpus():
  for i in range(len(df['Plot'])):
    sen = str(df['Plot'][i])
    sen = sen.strip()
    sen = sen.strip(' ')
    for j in range(1, 2):
      gg = str('Genre') + str(j)
      gnr = str(df[gg][i])
      gnr = gnr.lower()
      gnr = gnr.strip()
      gnr = gnr.strip(' ')
      if (gnr != "nan"):
        corpus.append(sen)
        ival = label_to_int[gnr]
        assert(ival >= 0 and ival < 22)
        corpus_label.append((ival, j * j))
make_corpus()

"""**TF-IDF + Deep Neural Network**"""

custom_count_vectorizer = Custom_Count_Vectorizer(corpus=corpus, corpus_label=corpus_label, vectorizer='TF-IDF')
dataset = custom_count_vectorizer.get_dataset(shuffle = True)

train_cnt = int(80.0 * len(dataset) / 100.0)
train_dataset = dataset[ : train_cnt]
test_dataset = dataset[train_cnt : ]
print('Train size =', len(train_dataset), '\nTest size =', len(test_dataset))

# Hyperparameters
batch_size =180 
num_iters =9000 
input_dim = len(dataset[0][0])
output_dim = len(all_genres)
learning_rate = 0.001

# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

num_epochs = num_iters / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)   # It's better to shuffle the whole training dataset! 

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False)  
print(num_epochs)

class DeepNeuralNetworkModel(nn.Module):
  def __init__(self, input_size, num_classes, num_hidden_1, num_hidden_2, num_hidden_3):
    super().__init__()
    ### 1st hidden layer: 784 --> 100
    self.linear_1 = nn.Linear(input_size, num_hidden_1)
    ### Non-linearity in 1st hidden layer
    self.relu_1 = nn.ReLU6()

    ### 2nd hidden layer: 100 --> 100
    self.linear_2 = nn.Linear(num_hidden_1, num_hidden_2)
    ### Non-linearity in 2nd hidden layer
    self.relu_2 = nn.ReLU6()

    self.linear_3 = nn.Linear(num_hidden_2, num_hidden_3)
    ### Non-linearity in 2nd hidden layer
    self.relu_3 = nn.ReLU6()

    ### Output layer: 100 --> 10
    self.linear_out = nn.Linear(num_hidden_3, num_classes)

  def forward(self, x):
    ### 1st hidden layer
    out  = self.linear_1(x)
    ### Non-linearity in 1st hidden layer
    out = self.relu_1(out)
    
    ### 2nd hidden layer
    out  = self.linear_2(out)
    ### Non-linearity in 2nd hidden layer
    out = self.relu_2(out)
    
    ### 2nd hidden layer
    out  = self.linear_3(out)
    ### Non-linearity in 2nd hidden layer
    out = self.relu_3(out)

    # Linear layer (output)
    probas  = self.linear_out(out)
    return probas

model = DeepNeuralNetworkModel(input_size = input_dim,
                               num_classes = output_dim,
                               num_hidden_1 = 550, num_hidden_2 = 250, num_hidden_3 = 80)
# To enable GPU
model.to(device)

# INSTANTIATE OPTIMIZER CLASS
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()

iteration_loss = []
iter = 0
for epoch in range(num_epochs):
  for i, (plots, labels) in enumerate(train_loader):

    plots = plots.view(-1, input_dim).to(device)
    labels = labels.to(device)

    # Clear gradients w.r.t. parameters
    optimizer.zero_grad()

    # Forward pass to get output/logits
    outputs = model(plots) 
    
    # Calculate Loss: softmax --> cross entropy loss
    loss = criterion(outputs, labels)

    # Getting gradients w.r.t. parameters
    loss.backward()

    # Updating parameters
    optimizer.step()

    iter += 1

    if iter % 500 == 0:
      # Calculate Accuracy         
      correct = 0
      total = 0
      # Iterate through test dataset
      for plots, labels in test_loader:
               
        plots = plots.view(-1, input_dim).to(device)

        # Forward pass only to get logits/output
        outputs = model(plots)

        # Get predictions from the maximum value
        _, predicted = torch.max(outputs, 1)

        # Total number of labels
        total += labels.size(0)


        # Total correct predictions
        if torch.cuda.is_available():
          correct += (predicted.cpu() == labels.cpu()).sum() 
          #print('cuda', predicted)
        else:
          correct += (predicted == labels).sum()
          #print('normal', predicted)

      accuracy = 100 * correct.item() / total
      # Print Loss
      iteration_loss.append(loss.item())
      print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))

#settings 
save_model = True

if save_model is True:
    # Saves only parameters
    # wights & biases
    torch.save(model.state_dict(), project_path + 'TF_IDF_DEEP_NN_model_final.pkl')

print (iteration_loss)
plt.plot(iteration_loss)
plt.ylabel('Cross Entropy Loss')
plt.xlabel('Iteration (in every 500)')
plt.show()

def new_prediction(plot):
  plot = custom_count_vectorizer.get_tensor(text)
  plot = plot.view(-1, input_dim).to(device)
  outputs = model(plot)
  _, predicted = torch.max(outputs, 1)
  return int(predicted)
  
def new_prediction_by_vector(vector):
  plot = vector
  plot = plot.view(-1, input_dim).to(device)
  outputs = model(plot)
  _, predicted = torch.max(outputs, 1)
  return int(predicted)

plot = "John McBut when a bomb goes off in the Bonwit Teller Department Store the police go insane trying to figure out what's going on. Soon, a man named Simon calls and asks for McClane. Simon tells Inspector Walter Cobb that McClane is going to play a game called \"Simon Says\". He says that McClane is going to do the tasks he assigns him. If not, he'll set off another bomb. With the help of a Harlem electrician, John McClane must race all over New York trying to figure out the frustrating puzzles that the crafty terrorist gives him. But when a bomb goes off in a subway station right by the Federal Reserve things start to get heated."
prediction=new_prediction(plot)
print(prediction,int_to_label[prediction])

out_true = []
out_pred = []
for plot in test_dataset:
  pred = new_prediction_by_vector(plot[0])
  out_true.append(plot[1])
  out_pred.append(pred)

cm = confusion_matrix(out_true, out_pred)
print(cm)
cr = classification_report(out_true, out_pred)
print(cr)

"""**TF-IDF + Logistic Regression**"""

custom_count_vectorizer = Custom_Count_Vectorizer(corpus=corpus, corpus_label=corpus_label, vectorizer='TF-IDF')
dataset = custom_count_vectorizer.get_dataset(shuffle=True)

train_cnt = int(80.0 * len(dataset) / 100.0)
train_dataset = dataset[ : train_cnt]
test_dataset = dataset[train_cnt : ]
print('Train size =', len(train_dataset), '\nTest size =', len(test_dataset))

# Hyperparameters
batch_size = 180
num_iters = 9000
input_dim = len(dataset[0][0])
output_dim = len(all_genres)
learning_rate = 0.001

# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

num_epochs = num_iters / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)   # It's better to shuffle the whole training dataset! 

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False)  
print(num_epochs)

class LogisticRegressionModel(nn.Module):
  def __init__(self, input_size, num_classes):
    super().__init__()
    self.linear = nn.Linear(input_size, num_classes)

  def forward(self, x):
    logits  = self.linear(x)
    probas = F.softmax(logits, dim=1)
    return logits, probas

'''
INSTANTIATE MODEL CLASS
'''
model = LogisticRegressionModel(input_size=input_dim,
                                num_classes=output_dim)
# To enable GPU
model.to(device)

# INSTANTIATE OPTIMIZER CLASS
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

'''
TRAIN THE MODEL
'''
iteration_loss = []
iter = 0
for epoch in range(num_epochs):
  for i, (plots, labels) in enumerate(train_loader):

    plots = plots.view(-1, input_dim).to(device)
    labels = labels.to(device)

    # Clear gradients w.r.t. parameters
    optimizer.zero_grad()

    # Forward pass to get output/logits
    logits, probas = model(plots) 

    # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities
    loss = F.cross_entropy(logits, labels)

    # Getting gradients w.r.t. parameters
    loss.backward()

    # Updating parameters
    optimizer.step()

    iter += 1

    if iter % 1000 == 0:
      # Calculate Accuracy         
      correct = 0
      total = 0
      # Iterate through test dataset
      for plots, labels in test_loader:
               
        plots = plots.view(-1, input_dim).to(device)

        # Forward pass only to get logits/output
        logits, probas = model(plots)

        # Get predictions from the maximum value
        _, predicted = torch.max(probas, 1)

        # Total number of labels
        total += labels.size(0)


        # Total correct predictions
        if torch.cuda.is_available():
          correct += (predicted.cpu() == labels.cpu()).sum() 
        else:
          correct += (predicted == labels).sum()

      accuracy = 100 * correct.item() / total

      # Print Loss
      iteration_loss.append(loss.item())
      print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))
print("done training")

#settings 
save_model = True

if save_model is True:
    # Saves only parameters
    # wights & biases
    torch.save(model.state_dict(), project_path + 'TF_IDF_logistic_regression_final.pkl')

print (iteration_loss)
plt.plot(iteration_loss)
plt.ylabel('Cross Entropy Loss')
plt.xlabel('Iteration (in every 500)')
plt.show()

def new_prediction_lg(plot):
  plot = custom_count_vectorizer.get_tensor(text)
  plots = plot.view(-1, input_dim).to(device)
  logits, probas = model(plots)
  _, predicted = torch.max(probas, 1)
  return int(predicted)
  
def new_prediction_by_vector_lg(vector):
  plot = vector
  plot = plot.view(-1, input_dim).to(device)
  logits, probas = model(plot)
  _, predicted = torch.max(probas, 1)
  return int(predicted)

plot = "John McBut when a bomb goes off in the Bonwit Teller Department Store the police go insane trying to figure out what's going on. Soon, a man named Simon calls and asks for McClane. Simon tells Inspector Walter Cobb that McClane is going to play a game called \"Simon Says\". He says that McClane is going to do the tasks he assigns him. If not, he'll set off another bomb. With the help of a Harlem electrician, John McClane must race all over New York trying to figure out the frustrating puzzles that the crafty terrorist gives him. But when a bomb goes off in a subway station right by the Federal Reserve things start to get heated."
prediction=new_prediction_lg(plot)
print(prediction,int_to_label[prediction])

out_true = []
out_pred = []
for plot in test_dataset:
  pred = new_prediction_by_vector_lg(plot[0])
  out_true.append(plot[1])
  out_pred.append(pred)

cm = confusion_matrix(out_true, out_pred)
print(cm)
cr = classification_report(out_true, out_pred)
print(cr)

"""**Binary Count + Deep Neural Netword**"""

custom_count_vectorizer = Custom_Count_Vectorizer(corpus=corpus, corpus_label=corpus_label, vectorizer='COUNT-B')
dataset = custom_count_vectorizer.get_dataset(shuffle = True)

train_cnt = int(80.0 * len(dataset) / 100.0)
train_dataset = dataset[ : train_cnt]
test_dataset = dataset[train_cnt : ]
print('Train size =', len(train_dataset), '\nTest size =', len(test_dataset))

# Hyperparameters
batch_size = 180
num_iters = 9000
input_dim = len(dataset[0][0])
output_dim = len(all_genres)
learning_rate = 0.001

# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

num_epochs = num_iters / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)   # It's better to shuffle the whole training dataset! 

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False)  
print(num_epochs)

class DeepNeuralNetworkModel(nn.Module):
  def __init__(self, input_size, num_classes, num_hidden_1, num_hidden_2, num_hidden_3):
    super().__init__()
    ### 1st hidden layer: 784 --> 100
    self.linear_1 = nn.Linear(input_size, num_hidden_1)
    ### Non-linearity in 1st hidden layer
    self.relu_1 = nn.ReLU6()

    ### 2nd hidden layer: 100 --> 100
    self.linear_2 = nn.Linear(num_hidden_1, num_hidden_2)
    ### Non-linearity in 2nd hidden layer
    self.relu_2 = nn.ReLU6()

    self.linear_3 = nn.Linear(num_hidden_2, num_hidden_3)
    ### Non-linearity in 2nd hidden layer
    self.relu_3 = nn.ReLU6()

    ### Output layer: 100 --> 10
    self.linear_out = nn.Linear(num_hidden_3, num_classes)

  def forward(self, x):
    ### 1st hidden layer
    out  = self.linear_1(x)
    ### Non-linearity in 1st hidden layer
    out = self.relu_1(out)
    
    ### 2nd hidden layer
    out  = self.linear_2(out)
    ### Non-linearity in 2nd hidden layer
    out = self.relu_2(out)
    
    ### 2nd hidden layer
    out  = self.linear_3(out)
    ### Non-linearity in 2nd hidden layer
    out = self.relu_3(out)

    # Linear layer (output)
    probas  = self.linear_out(out)
    return probas

model = DeepNeuralNetworkModel(input_size = input_dim,
                               num_classes = output_dim,
                               num_hidden_1 = 550, num_hidden_2 = 250, num_hidden_3 = 80)
# To enable GPU
model.to(device)

# INSTANTIATE OPTIMIZER CLASS
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()

iteration_loss = []
iter = 0
for epoch in range(num_epochs):
  for i, (plots, labels) in enumerate(train_loader):

    plots = plots.view(-1, input_dim).to(device)
    labels = labels.to(device)

    # Clear gradients w.r.t. parameters
    optimizer.zero_grad()

    # Forward pass to get output/logits
    outputs = model(plots) 
    
    # Calculate Loss: softmax --> cross entropy loss
    loss = criterion(outputs, labels)

    # Getting gradients w.r.t. parameters
    loss.backward()

    # Updating parameters
    optimizer.step()

    iter += 1

    if iter % 500 == 0:
      # Calculate Accuracy         
      correct = 0
      total = 0
      # Iterate through test dataset
      for plots, labels in test_loader:
               
        plots = plots.view(-1, input_dim).to(device)

        # Forward pass only to get logits/output
        outputs = model(plots)

        # Get predictions from the maximum value
        _, predicted = torch.max(outputs, 1)

        # Total number of labels
        total += labels.size(0)


        # Total correct predictions
        if torch.cuda.is_available():
          correct += (predicted.cpu() == labels.cpu()).sum() 
          #print('cuda', predicted)
        else:
          correct += (predicted == labels).sum()
          #print('normal', predicted)

      accuracy = 100 * correct.item() / total
      # Print Loss
      iteration_loss.append(loss.item())
      print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))

#settings 
save_model = True

if save_model is True:
    # Saves only parameters
    # wights & biases
    torch.save(model.state_dict(), project_path + 'binary_count_DEEP_NN_model_final.pkl')

print (iteration_loss)
plt.plot(iteration_loss)
plt.ylabel('Cross Entropy Loss')
plt.xlabel('Iteration (in every 500)')
plt.show()

def new_prediction(plot):
  plot = custom_count_vectorizer.get_tensor(text)
  plot = plot.view(-1, input_dim).to(device)
  outputs = model(plot)
  _, predicted = torch.max(outputs, 1)
  return int(predicted)
  
def new_prediction_by_vector(vector):
  plot = vector
  plot = plot.view(-1, input_dim).to(device)
  outputs = model(plot)
  _, predicted = torch.max(outputs, 1)
  return int(predicted)

plot = "John McBut when a bomb goes off in the Bonwit Teller Department Store the police go insane trying to figure out what's going on. Soon, a man named Simon calls and asks for McClane. Simon tells Inspector Walter Cobb that McClane is going to play a game called \"Simon Says\". He says that McClane is going to do the tasks he assigns him. If not, he'll set off another bomb. With the help of a Harlem electrician, John McClane must race all over New York trying to figure out the frustrating puzzles that the crafty terrorist gives him. But when a bomb goes off in a subway station right by the Federal Reserve things start to get heated."
prediction=new_prediction(plot)
print(prediction,int_to_label[prediction])

out_true = []
out_pred = []
for plot in test_dataset:
  pred = new_prediction_by_vector(plot[0])
  out_true.append(plot[1])
  out_pred.append(pred)

cm = confusion_matrix(out_true, out_pred)
print(cm)
cr = classification_report(out_true, out_pred)
print(cr)

"""**Binary Count + Logistic Regression**"""

custom_count_vectorizer = Custom_Count_Vectorizer(corpus=corpus, corpus_label=corpus_label, vectorizer='COUNT-B')
dataset = custom_count_vectorizer.get_dataset(shuffle=True)

train_cnt = int(80.0 * len(dataset) / 100.0)
train_dataset = dataset[ : train_cnt]
test_dataset = dataset[train_cnt : ]
print('Train size =', len(train_dataset), '\nTest size =', len(test_dataset))

# Hyperparameters
batch_size = 220
num_iters = 6000
input_dim = len(dataset[0][0])
output_dim = len(all_genres)
learning_rate = 0.001

# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

num_epochs = num_iters / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)   # It's better to shuffle the whole training dataset! 

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False)  
print(num_epochs)

class LogisticRegressionModel(nn.Module):
  def __init__(self, input_size, num_classes):
    super().__init__()
    self.linear = nn.Linear(input_size, num_classes)

  def forward(self, x):
    logits  = self.linear(x)
    probas = F.softmax(logits, dim=1)
    return logits, probas

'''
INSTANTIATE MODEL CLASS
'''
model = LogisticRegressionModel(input_size=input_dim,
                                num_classes=output_dim)
# To enable GPU
model.to(device)

# INSTANTIATE OPTIMIZER CLASS
optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)

'''
TRAIN THE MODEL
'''
iteration_loss = []
iter = 0
for epoch in range(num_epochs):
  for i, (plots, labels) in enumerate(train_loader):

    plots = plots.view(-1, input_dim).to(device)
    labels = labels.to(device)

    # Clear gradients w.r.t. parameters
    optimizer.zero_grad()

    # Forward pass to get output/logits
    logits, probas = model(plots) 

    # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities
    loss = F.cross_entropy(logits, labels)

    # Getting gradients w.r.t. parameters
    loss.backward()

    # Updating parameters
    optimizer.step()

    iter += 1

    if iter % 1000 == 0:
      # Calculate Accuracy         
      correct = 0
      total = 0
      # Iterate through test dataset
      for plots, labels in test_loader:
               
        plots = plots.view(-1, input_dim).to(device)

        # Forward pass only to get logits/output
        logits, probas = model(plots)

        # Get predictions from the maximum value
        _, predicted = torch.max(probas, 1)

        # Total number of labels
        total += labels.size(0)


        # Total correct predictions
        if torch.cuda.is_available():
          correct += (predicted.cpu() == labels.cpu()).sum() 
        else:
          correct += (predicted == labels).sum()

      accuracy = 100 * correct.item() / total

      # Print Loss
      iteration_loss.append(loss.item())
      print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))
print("done training")

#settings 
save_model = True

if save_model is True:
    # Saves only parameters
    # wights & biases
    torch.save(model.state_dict(), project_path + 'binary_count_logistic_final.pkl')

print (iteration_loss)
plt.plot(iteration_loss)
plt.ylabel('Cross Entropy Loss')
plt.xlabel('Iteration (in every 500)')
plt.show()

def new_prediction_lg(plot):
  plot = custom_count_vectorizer.get_tensor(text)
  plots = plot.view(-1, input_dim).to(device)
  logits, probas = model(plots)
  _, predicted = torch.max(probas, 1)
  return int(predicted)
  
def new_prediction_by_vector_lg(vector):
  plot = vector
  plot = plot.view(-1, input_dim).to(device)
  logits, probas = model(plot)
  _, predicted = torch.max(probas, 1)
  return int(predicted)

plot = "John McBut when a bomb goes off in the Bonwit Teller Department Store the police go insane trying to figure out what's going on. Soon, a man named Simon calls and asks for McClane. Simon tells Inspector Walter Cobb that McClane is going to play a game called \"Simon Says\". He says that McClane is going to do the tasks he assigns him. If not, he'll set off another bomb. With the help of a Harlem electrician, John McClane must race all over New York trying to figure out the frustrating puzzles that the crafty terrorist gives him. But when a bomb goes off in a subway station right by the Federal Reserve things start to get heated."
prediction=new_prediction_lg(plot)
print(prediction,int_to_label[prediction])

out_true = []
out_pred = []
for plot in test_dataset:
  pred = new_prediction_by_vector_lg(plot[0])
  out_true.append(plot[1])
  out_pred.append(pred)

cm = confusion_matrix(out_true, out_pred)
print(cm)
cr = classification_report(out_true, out_pred)
print(cr)

"""**Count + Deep Neural Network**"""

custom_count_vectorizer = Custom_Count_Vectorizer(corpus=corpus, corpus_label=corpus_label, vectorizer='COUNT')
dataset = custom_count_vectorizer.get_dataset(shuffle = True)

train_cnt = int(80.0 * len(dataset) / 100.0)
train_dataset = dataset[ : train_cnt]
test_dataset = dataset[train_cnt : ]
print('Train size =', len(train_dataset), '\nTest size =', len(test_dataset))

# Hyperparameters
batch_size = 200
num_iters = 9000
input_dim = len(dataset[0][0])
output_dim = len(all_genres)
learning_rate = 0.001

# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

num_epochs = num_iters / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)   # It's better to shuffle the whole training dataset! 

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False)  
print(num_epochs)

class DeepNeuralNetworkModel(nn.Module):
  def __init__(self, input_size, num_classes, num_hidden_1, num_hidden_2, num_hidden_3):
    super().__init__()
    ### 1st hidden layer: 784 --> 100
    self.linear_1 = nn.Linear(input_size, num_hidden_1)
    ### Non-linearity in 1st hidden layer
    self.relu_1 = nn.ReLU6()

    ### 2nd hidden layer: 100 --> 100
    self.linear_2 = nn.Linear(num_hidden_1, num_hidden_2)
    ### Non-linearity in 2nd hidden layer
    self.relu_2 = nn.ReLU6()

    self.linear_3 = nn.Linear(num_hidden_2, num_hidden_3)
    ### Non-linearity in 2nd hidden layer
    self.relu_3 = nn.ReLU6()

    ### Output layer: 100 --> 10
    self.linear_out = nn.Linear(num_hidden_3, num_classes)

  def forward(self, x):
    ### 1st hidden layer
    out  = self.linear_1(x)
    ### Non-linearity in 1st hidden layer
    out = self.relu_1(out)
    
    ### 2nd hidden layer
    out  = self.linear_2(out)
    ### Non-linearity in 2nd hidden layer
    out = self.relu_2(out)
    
    ### 2nd hidden layer
    out  = self.linear_3(out)
    ### Non-linearity in 2nd hidden layer
    out = self.relu_3(out)

    # Linear layer (output)
    probas  = self.linear_out(out)
    return probas

model = DeepNeuralNetworkModel(input_size = input_dim,
                               num_classes = output_dim,
                               num_hidden_1 = 550, num_hidden_2 = 256, num_hidden_3 = 64)
# To enable GPU
model.to(device)

# INSTANTIATE OPTIMIZER CLASS
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()

iteration_loss = []
iter = 0
for epoch in range(num_epochs):
  for i, (plots, labels) in enumerate(train_loader):

    plots = plots.view(-1, input_dim).to(device)
    labels = labels.to(device)

    # Clear gradients w.r.t. parameters
    optimizer.zero_grad()

    # Forward pass to get output/logits
    outputs = model(plots) 
    
    # Calculate Loss: softmax --> cross entropy loss
    loss = criterion(outputs, labels)

    # Getting gradients w.r.t. parameters
    loss.backward()

    # Updating parameters
    optimizer.step()

    iter += 1

    if iter % 500 == 0:
      # Calculate Accuracy         
      correct = 0
      total = 0
      # Iterate through test dataset
      for plots, labels in test_loader:
               
        plots = plots.view(-1, input_dim).to(device)

        # Forward pass only to get logits/output
        outputs = model(plots)

        # Get predictions from the maximum value
        _, predicted = torch.max(outputs, 1)

        # Total number of labels
        total += labels.size(0)


        # Total correct predictions
        if torch.cuda.is_available():
          correct += (predicted.cpu() == labels.cpu()).sum() 
          #print('cuda', predicted)
        else:
          correct += (predicted == labels).sum()
          #print('normal', predicted)

      accuracy = 100 * correct.item() / total
      # Print Loss
      iteration_loss.append(loss.item())
      print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))

#settings 
save_model = True

if save_model is True:
    # Saves only parameters
    # wights & biases
    torch.save(model.state_dict(), project_path + 'Count_DEEP_NN_model_final.pkl')

print (iteration_loss)
plt.plot(iteration_loss)
plt.ylabel('Cross Entropy Loss')
plt.xlabel('Iteration (in every 500)')
plt.show()

def new_prediction(plot):
  plot = custom_count_vectorizer.get_tensor(text)
  plot = plot.view(-1, input_dim).to(device)
  outputs = model(plot)
  _, predicted = torch.max(outputs, 1)
  return int(predicted)
  
def new_prediction_by_vector(vector):
  plot = vector
  plot = plot.view(-1, input_dim).to(device)
  outputs = model(plot)
  _, predicted = torch.max(outputs, 1)
  return int(predicted)

plot = "John McBut when a bomb goes off in the Bonwit Teller Department Store the police go insane trying to figure out what's going on. Soon, a man named Simon calls and asks for McClane. Simon tells Inspector Walter Cobb that McClane is going to play a game called \"Simon Says\". He says that McClane is going to do the tasks he assigns him. If not, he'll set off another bomb. With the help of a Harlem electrician, John McClane must race all over New York trying to figure out the frustrating puzzles that the crafty terrorist gives him. But when a bomb goes off in a subway station right by the Federal Reserve things start to get heated."
prediction=new_prediction(plot)
print(prediction,int_to_label[prediction])

out_true = []
out_pred = []
for plot in test_dataset:
  pred = new_prediction_by_vector(plot[0])
  out_true.append(plot[1])
  out_pred.append(pred)

cm = confusion_matrix(out_true, out_pred)
print(cm)
cr = classification_report(out_true, out_pred)
print(cr)

"""** Count + Logistic Regression **"""

custom_count_vectorizer = Custom_Count_Vectorizer(corpus=corpus, corpus_label=corpus_label, vectorizer='COUNT')
dataset = custom_count_vectorizer.get_dataset(shuffle=True)

train_cnt = int(80.0 * len(dataset) / 100.0)
train_dataset = dataset[ : train_cnt]
test_dataset = dataset[train_cnt : ]
print('Train size =', len(train_dataset), '\nTest size =', len(test_dataset))

# Hyperparameters
batch_size = 200
num_iters = 9000
input_dim = len(dataset[0][0])
output_dim = len(all_genres)
learning_rate = 0.001

# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

num_epochs = num_iters / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)   # It's better to shuffle the whole training dataset! 

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False)  
print(num_epochs)

class LogisticRegressionModel(nn.Module):
  def __init__(self, input_size, num_classes):
    super().__init__()
    self.linear = nn.Linear(input_size, num_classes)

  def forward(self, x):
    logits  = self.linear(x)
    probas = F.softmax(logits, dim=1)
    return logits, probas

'''
INSTANTIATE MODEL CLASS
'''
model = LogisticRegressionModel(input_size=input_dim,
                                num_classes=output_dim)
# To enable GPU
model.to(device)

# INSTANTIATE OPTIMIZER CLASS
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

'''
TRAIN THE MODEL
'''
iteration_loss = []
iter = 0
for epoch in range(num_epochs):
  for i, (plots, labels) in enumerate(train_loader):

    plots = plots.view(-1, input_dim).to(device)
    labels = labels.to(device)

    # Clear gradients w.r.t. parameters
    optimizer.zero_grad()

    # Forward pass to get output/logits
    logits, probas = model(plots) 

    # Calculate Loss: PyTorch implementation of CrossEntropyLoss works with logits, not probabilities
    loss = F.cross_entropy(logits, labels)

    # Getting gradients w.r.t. parameters
    loss.backward()

    # Updating parameters
    optimizer.step()

    iter += 1

    if iter % 1000 == 0:
      # Calculate Accuracy         
      correct = 0
      total = 0
      # Iterate through test dataset
      for plots, labels in test_loader:
               
        plots = plots.view(-1, input_dim).to(device)

        # Forward pass only to get logits/output
        logits, probas = model(plots)

        # Get predictions from the maximum value
        _, predicted = torch.max(probas, 1)

        # Total number of labels
        total += labels.size(0)


        # Total correct predictions
        if torch.cuda.is_available():
          correct += (predicted.cpu() == labels.cpu()).sum() 
        else:
          correct += (predicted == labels).sum()

      accuracy = 100 * correct.item() / total

      # Print Loss
      iteration_loss.append(loss.item())
      print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))
print("done training")

#settings 
save_model = True

if save_model is True:
    # Saves only parameters
    # wights & biases
    torch.save(model.state_dict(), project_path + 'Count_logistic_regression_final.pkl')

print (iteration_loss)
plt.plot(iteration_loss)
plt.ylabel('Cross Entropy Loss')
plt.xlabel('Iteration (in every 500)')
plt.show()

def new_prediction_lg(plot):
  plot = custom_count_vectorizer.get_tensor(text)
  plots = plot.view(-1, input_dim).to(device)
  logits, probas = model(plots)
  _, predicted = torch.max(probas, 1)
  return int(predicted)
  
def new_prediction_by_vector_lg(vector):
  plot = vector
  plot = plot.view(-1, input_dim).to(device)
  logits, probas = model(plot)
  _, predicted = torch.max(probas, 1)
  return int(predicted)

plot = "John McBut when a bomb goes off in the Bonwit Teller Department Store the police go insane trying to figure out what's going on. Soon, a man named Simon calls and asks for McClane. Simon tells Inspector Walter Cobb that McClane is going to play a game called \"Simon Says\". He says that McClane is going to do the tasks he assigns him. If not, he'll set off another bomb. With the help of a Harlem electrician, John McClane must race all over New York trying to figure out the frustrating puzzles that the crafty terrorist gives him. But when a bomb goes off in a subway station right by the Federal Reserve things start to get heated."
prediction=new_prediction_lg(plot)
print(prediction,int_to_label[prediction])

out_true = []
out_pred = []
for plot in test_dataset:
  pred = new_prediction_by_vector_lg(plot[0])
  out_true.append(plot[1])
  out_pred.append(pred)

cm = confusion_matrix(out_true, out_pred)
print(cm)
cr = classification_report(out_true, out_pred)
print(cr)

